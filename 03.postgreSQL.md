# NOTES: PostgreSQL 

---

## Table of contents
  + [1. What is postgreSQL](#what-is-postgresql)
  + [2. installation](#installation)
  + [3. restoring a database](#restoring-a-database)
  + [4. understanding the internals of postgreSQL](#understanding-the-internals-of-postgresql)
  + [5. indexes](#indexes)
  + [6. query tuning](#query-tuning)
  + [7. advanced query tuning](#advanced-query-tuning)
  + [8. schema migration](#schema-migration)
  + [9. data migration](#data-migration)


---

## What is PostgreSQL

* Installing postgreSQL installs pgadmin.
* Pgadmin is a web-based tool to manage and inspect a postgres database.
* One postgres server can have multiple databases.
* One app usually associated with one database.
* use Query Editor to test SQL on a specific database
* view table definition by clicking on database -> expanding schemas -> tables
  (right click on table) -> view/edit data -> all rows

## Installation

* installation https://www.enterprisedb.com/downloads/postgres-postgresql-downloads
* during install -> uncheck stack builder
* password is password for local machine
* run pgAdmin 4 -> password is to access pgadmin
* clicking on servers -> requested password is password for localmachine.
* have to refresh table to see updated info.

---
## Restoring a database

#### restoring database from a backup
sql databases backup to a .sql file.

restore from backup:
pgadmin -> servers-> localhost -> databases -> instagram -> *(right click) -> restore
filename -> ... -> *(format -> all files or .sql) -> *(select file to restore from)

might have to set the 'PostgreSQL Binary Path':
In pgAdmin select File -> Preferences and look for Path and then click on Binary Path and it needs your path where it says PostgreSQL Binary Path. Go to your computer -> C: (on windows) -> Program Files -> PostgreSQL -> your version -> bin 

restore options tab -> enable: 
	1. types of objects -> only data -> yes
	2. do not save -> owner -> yes
	3. queries -> single transaction -> yes
	4. disable -> trigger -> yes
	5. miscelleaneous / behavior -> verbose messages -> yes

#### restore database from scratch 
steps to restore:
1. close all query tools windows
2. stop all server activity except the first under pgadmin -> dashboard
3. drop the database
4. create new database
5. restore from backup (same step as above) BUT restore options tab -> enable ONLY 4 steps from above

restore from backup:
pgadmin -> servers-> localhost -> databases -> instagram -> *(right click) -> restore
filename -> ... -> *(format -> all files or .sql) -> *(select file to restore from)

1. types of objects -> only data -> NO

2. do not save -> owner -> yes
3. queries -> single transaction -> yes
4. disable -> trigger -> yes
5. miscelleaneous / behavior -> verbose messages -> yes

---
###### <div style="text-align:right">[table of contents](#table-of-contents)</div>

## Understanding the Internals of PostgreSQL

#### where postgreSQL stores data
```SQL
show data_directory;
```
this gives us a path like: /users/*username*/library/application support/postgres/var-12
all database content is stored in 'base' folder

#### Folder where database is stored
results are table(oid, datname).
the oid is the internal identifier (which is also the folder name inside the 'base' folder) that is associated with a database
```SQL
SELECT oid, datname
FROM pg_database;
```

#### file where database table is stored
inside that folder, each file represents one object inside database
table (oid, relname, relnamespace, reltype ...etc)
```SQL
SELECT * FROM pg_class;
```
each row represents an object refers to eg. tables, indexes, primary keys.
so find the table we want and look at associated 'oid', 
that will be name of file which will contain all data for that object. eg. 22445

* Heap file - file that contains all the data (rows) of our table
* page or block - heap is divided into many different 'blocks' or 'pages'. Each page/block stores a number of rows
* tuple or item - individual row from the table.

Heap file is divided up into many page/blocks and each page/block has many tuple/item/rows.
each block is 8kb.

* Block is binary data (stores 0's and 1's) and divided up into parts. 
* begining part stores information about block, 
* then next few blocks stores information about rows, where to find data.
* free space
* actual data for tuples.

#### deciphering a page/block

[postgreSQL docs about page/block](http://postgresql.org/docs/current/storage-page-layout.html)
* Table 68.2 Overall Page layout

* view with hex reader (vs code extensions - microsoft hex editor)
* open the file with the hex editor (CTRL+SHIFT+P) -> hex editor
* hex code is actual "shortcut" or encoded binary 1's and 0's
* each hex is 1 byte.

#### Page Layout
* PageHeaderData

##### PageHeaderData 
24 bytes long. contains information about page
Table 68.3 PageHeaderData layout
* pd_lsn - 8 bytes
* pd_checksum - 2 bytes
* pd_flags - 2 bytes
* pd_lower - 2 bytes (*offset to start of free space)
* pd_upper - 2 bytes (*offset to end of free space)
* pd_special - 2 bytes
* pd_pagesize_version
* pd_prune_xid

pd_lower:
Once we know where pd_lower is located - we know the number of bytes starting at beginning of page to start off free space.
clicking on first byte -> then in hex editor (Data inspector) - pd_lower - Int16 value -> eg. 228
228 means we count off 228 bytes from start of page/block (228 / 16 (each row has 16) = 14.25), next value is the start of free space

pd_higher:
2 bytes - clicking on first byte -> data inspector -> int16 -> value is 296.
296 gives value for end of free space area -> 296/16(each row in hex editor has 16 cols) = 18.5
counting down from begining of block to 18.5, we will find the end of free space.

##### ItemIdData
4 bytes per item. array of identifiers pointing to actual items. 
There are multiple of these after the page header.

how data is stored:
click on start of 4 bytes, then go to the next byte (2nd of 4 bytes).
Data Inspector -> look at 8 bit binary eg. 10110100
look at the 2nd digit of the 8bit binary.
find binary to decimal converter: 0110100
then go back and copy the entire 8 bit binary 10110100

so in our example, concat 0110100 + 10110100 and convert to decimal = eg. 296 
this will give you a decimal number which is the number of bytes from the start of the page to the first item.
296 / 16 = 18.5 rows. and that will be the start of the first item.


##### Freespace - unlocated space.

##### Items - actual items themselves. 
to calculate length of data. click on 3rd byte of 4 the bytes. look at int16 value. eg. 172
length of first item is 172. 

68.6.1 Table row layout - And item is further made up of a fixed-size header (23 bytes), followed by optional data, then is the actual user data.
the pointer to the first value -> data inspector -> Int16 -> eg. 203 is the actual data is the id of the data in the postgreSQL table.

---
###### <div style="text-align:right">[table of contents](#table-of-contents)</div>

## Indexes
Indexes help with performance
* without indexes, to search data the data has to be loaded up to memory (large performance cost)
* then search through all data for what we are looking for (iteration of rows) also know as 'Full table scan'
* indexes are data structures that efficiently tells us which block and index a record is stored in a heap file on harddrive.

#### how an index works?
* basically how index works is it picks a column we want to have a fast look up on.
* you can have an index on multiple columns. 
* then we look at our heap file and that property we want to extract for every row and we then record the block and index and that property (column) value.
* we then sort in some meaningful way, eg. alphabetically.
* put this list of data into a binary tree
* root node gets instructions on where the new node should go eg. left or right (elimination of wrong node direction)
* we cut down the number of blocks we have to read data from.

#### creating an index
* creating an index on users table -> username column.
* view all indexes for a database -> schemas -> table -> users -> indexes -> *(refresh) -> 
* naming convention: users_username_idx

```SQL
CREATE INDEX on users (username);
```

#### deleting index
```SQL
DROP INDEX users_username_idx;
```

#### Benchmarking queries with indexes
Keyword : 'EXPLAIN ANALYZE' in front of SELECT statement
Indexes make queries run 16X faster

```SQL
EXPLAIN ANALYZE SELECT * 
FROM users
WHERE username = 'Emil30';
```

#### downside of indexes
Indexes are performance benefit, however it can sometimes slow down database.
Indexes take up storage space.

```SQL
--Finding out how much space index use.
SELECT pg_size_pretty(pg_relation_size('users_username_idx'))
```

Indexes slow down insert/update/delete because index has to be updated (especially tables that get updated frequently)

Index might not get used. 

#### PostgreSQL autogenerated indexes
postgres does manage its own indexes.
Idexes are created automatically by postgresql for columns (and therefore you never have to create your own indexes for these):

* primary keys
* column that has UNIQUE constraint

Query to see which indexes (relkind = i) actually exist for database 

```SQL
SELECT relname, relkind
FROM pg_class
WHERE relkind = 'i'  -- type: index
```

---
###### <div style="text-align:right">[table of contents](#table-of-contents)</div>

## Query tuning
When postgres receives query
1. goes into parser: splits up the query and figures out what each SQL keyword means
2. after evaluation, it builds a query tree (programatic tree of query)
3. rewriter takes tree and makes adjustments to it.
4. planner takes a look at query tree and strategizes to get that information.
5. planner picks a strategy 
6. executer executes strategy


keywords:

* EXPLAIN - build query plan and display info about it. (plan)

* EXPLAIN ANALYZE - build query plan, RUN IT!, and display info about it. (plan + run)

both keywords are only used for performance optimization and never for production.
pgadmin has a explain analyze button (make sure all options are checked).
How to read explain analyze output:
go to deepest level of query node of query plan (most indented), we can imagine that they keep passing information up to the nearest parent node.

```SQL
SELECT * 
FROM pg_stats
WHERE tablename = 'users';
```

postgres is able to make asumptions of (rows, width) in output of EXPLAIN ANALYZE because it actually keeps stats about whats going on in the database.

##### Cost
EXPLAIN ANALYZE -> cost
amount of time to execute a part of a query.
* query plan has something like (cost=9.4...1233.11) 
	- 9.4 is the cost to calculate the first row value
	- 1233.11 is the cost to calculate all rows
* cost of parent node in query plan is the sum of all child nodes' costs.

---

###### <div style="text-align:right">[table of contents](#table-of-contents)</div>

## Advanced Query tuning 

using index vs loading all up in memory and reading sequencially:

* assumption that jumping to specific blocks/random child page is 4x as long (eg. x2 pages loaded) = 4 x 2 = 8 units

* compared to sequencial reading x1 (assume 1x base value) which is loading for each page x 110 per file to search through list = 110 x 1 = 110 units

eg. EXPLAIN ANALYZE - 

Seq Scan on comments (cost=0.00...1589.10 rows=60410 width=72) (actual time = 0.008...14.29). 60410 rows, 985 pages.

loading up a page to memory is more expensive than sequencial reading of rows
attempting formula:

1.0 (assume score is 1.0 as base to judge everything else in terms of estimate costs)
0.01  (assume loading row is 1% cost of loading up row)

(# pages) * 1.0 + (# rows) * (0.01)

(985) * 1.0 + (60410) * (0.01) = 1589.1 (same estimate as EXPLAIN ANALYZE execution)

##### actual formula for cost for any step of query plan (EXPLAIN ANALYZE)

link specifics default values for costs to calculations. sequencial base cost (seq_page_cost) is the default and all other costs are relative to that.

[runtime config query - postgresql.org/docs/current/runtime-config-query.html](http://postgresql.org/docs/current/runtime-config-query.html) 
19.7 Query Planning -> 19.7.2 Planner cost constants

random_page_cost -> 4x more expensive as fetching page in order (seq_page_cost)

seq_page_cost -> 1x (default base that all other costs relative to)

cpu_tuple_cost -> 0.1 (processing a single tuple (row) is 1% as expensive as fetching a page in order (seq_page_cost))

cpu_index_tuple_cost - 0.005 (processing a tuple from an index is 50% as expensive as processing a real row (cpu_tuple_cost))

cpu_operator_cost - 0.0025 (running an operator or function is 50% as expensive as processing an index tuple (cpu_index_tuple_cost))

##### Cost for steps of query plan = 
(# pages read sequentially) * seq_page_cost
+
(# page read at random) * random_page_cost
+
(# rows scanned) * cpu_tuple_cost
+
(# index entries scanned) * cpu_index_tuple_cost
+
(# times function/operator evaluated) * cpu_operator_cost


Cost for sequential read = 
(# pages read sequentially) * seq_page_cost
+ 
(# rows scanned) * cpu_tuple_cost

---

###### <div style="text-align:right">[table of contents](#table-of-contents)</div>

## Schema Migration
Have to be careful and plan of making changes to databases.  
1. changes to database and changes to clients (api server) need to be made at precisely the same time.

2. When working with other engineers, we need a really easy way to tie the structure of our database to our code. 
ie. when they check out a version of your code, their databases has a way of being insync with this code.

### Migration files
no more changes to database directly in pgadmin, will use Schema Migration Files.
Schema migration files can be written in any language.
it has an 'UP' and 'DOWN' section.
the up upgrades the structure of our database - apply
the down downgrade and undo's the 'UP' commands - revert
every migration file has what you need to make a change, and undo a change.

Steps to migration:
1. start deploy of updates
2. new version of code on remote server, ready to start receiving traffic.
    run all available migrations - db structure updated.
3. start receiving traffic

It is suggested to write migrations manually.

### migration libraries
Javascript 
npmjs.com/package/node-pg-migrate
npmjs.com/package/typeorm
npmjs.com/package/sequelize
npmjs.com/package/db-migrate

Go
github.com/golang-migrate/migrate
github.com/go-pg/migrations/
gorm.io/docs/migration.html

Python
pypi.org/project/alembic/
pypi.org/project/yoyo-migrations/

#### npmjs.com/package/node-pg-migrate
* required: node.js
* npm install node-pg-migrate pg

```js
//package.js
"scripts": {
  "migrate":"node-pg-migrate"
}
```

##### create migration file
* migration file will be added to migrations/ folder.
* creates a .js file files are named with timestamp, tells which order they should be run.

```cmd
npm run migrate create table comments
```

##### documentation example in javascript with object like syntax
```js
exports.up = (pgm) => {
  pgm.createTable('users', {
    id: 'id',
    name: { type: 'varchar(1000)', notNull: true },
    createdAt: {
      type: 'timestamp',
      notNull: true,
      default: pgm.func('current_timestamp'),
    },
  })
  pgm.createTable('posts', {
    id: 'id',
    userId: {
      type: 'integer',
      notNull: true,
      references: '"users"',
      onDelete: 'cascade',
    },
    body: { type: 'text', notNull: true },
    createdAt: {
      type: 'timestamp',
      notNull: true,
      default: pgm.func('current_timestamp'),
    },
  })
  pgm.createIndex('posts', 'userId')
}
```
##### documentation example in javascript using SQL
```js
exports.shorthands = undefined;
exports.up = pgm => {
  pgm.sql(`
    CREATE TABLE comments (
      id SERIAL PRIMARY KEY,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
      updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
      contents VARCHAR(200) NOT NULL
    );
  `);
};

exports.down = pgm = {
  pgm.sql(`
    DROP TABLE comments;
  `);
};
```

##### view executed migrations
Schemas->public->tables->pgmigrations->view/edit data -> all rows

##### Running migration commands
set up Environment variable: DATABASE_URL
where 'socialnetwork' is the name of the database.
to run the down command, replace 'up' with 'down', which udoes the previous 'up' migration.

to this value:
```
postgres:// USERNAME : PASSWORD @localhost:5432/socialnetwork
```

###### on windows
username = postgres
password = you set this when you installed PostgreSQL

###### on mac
username = your username
password = none

Windows with Git Bash
```cmd
DATABASE_URL=postgres://USERNAME:PASSWORD@localhost:5432/socialnetwork npm run migrate up
```

Windows with CMD
```cmd
set DATABASE_URL=postgres://USERNAME:PASSWORD@localhost:5432/socialnetwork&&npm run migrate up
```

Windows with Powershell
```cmd
$env:DATABASE_URL="postgres://USERNAME:PASSWORD@localhost:5432/socialnetwork"; npm run migrate up
```

MacOS
 ```cmd
DATABASE_URL=postgres://USERNAME@localhost:5432/socialnetwork npm run migrate up
 ```

##### running a second migration file
changing the 'contents' column to 'body'
(creates a migration .js file)

```cmd
npm run migrate create rename contents to body
```

```js
exports.shorthands = undefined;

exports.up = pgm => {
  pgm.sql('ALTER TABLE comments RENAME COLUMN contents TO body');
};

exports.down = pgm => {
  pg.sql('ALTER TABLE comments RENAME COLUMN body TO contents');
};
```

then re-run the migrate up command.

calling the down migration will revert one migration file at a time.

---

###### <div style="text-align:right">[table of contents](#table-of-contents)</div>

## Data Migration

---